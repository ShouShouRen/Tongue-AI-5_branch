# train_distill.py - çŸ¥è­˜è’¸é¤¾å°ˆç”¨è¨“ç·´è…³æœ¬
# --- è¨“ç·´ä¸€å€‹è¼•é‡ç´š Student æ¨¡å‹ä¾†æ¨¡ä»¿ Teacher æ¨¡å‹ ---

import os
import gc
import json
import warnings
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch import optim
from tqdm import tqdm
import numpy as np
import pandas as pd
import argparse
import copy

# å¾æ¨¡å‹åº«åŒ¯å…¥æ‰€æœ‰æ¨¡å‹
from model import (
    SignOrientedNetwork, 
    SimpleTimmModel, 
    SignOrientedAttentionNetwork,
    SignOrientedMambaVisionNetwork,
    SimpleMambaVision,
    MambaVisionWithMambaFusion
)
from dataset import TongueDataset
from samplers import build_multilabel_weighted_sampler
from torch.cuda.amp import autocast, GradScaler
from sklearn.metrics import f1_score, accuracy_score

# å˜—è©¦è¼‰å…¥è‡ªå®šç¾©æå¤±å‡½æ•¸
try:
    from losses import ClassBalancedBCELoss
except ImportError:
    print("âš ï¸ ClassBalancedBCELoss not found, using standard BCEWithLogitsLoss")
    ClassBalancedBCELoss = None

warnings.filterwarnings('ignore')
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'

# --- è¤‡è£½è‡ª train.py ---
# ç‚ºäº†è®“æ­¤è…³æœ¬ç¨ç«‹é‹ä½œï¼Œæˆ‘å€‘å®Œæ•´è¤‡è£½ get_model
def get_model(args):
    """æ ¹æ“š args å»ºç«‹å°æ‡‰çš„æ¨¡å‹"""
    
    # SimpleTimmModel - å–®åˆ†æ”¯ CNN
    if args.model == 'Simple':
        return SimpleTimmModel(
            num_classes=len(args.label_cols),
            backbone=args.backbone,
            img_size=args.img_size
        )
    
    # SignOriented - åŸå§‹äº”åˆ†æ”¯æ¨¡å‹
    elif args.model == 'SignOriented':
        return SignOrientedNetwork(
            num_classes=len(args.label_cols),
            backbone=args.backbone,
            feature_dim=args.feature_dim
        )
    
    # SignOrientedAttention - Transformer Attention èåˆ
    elif args.model == 'SignOrientedAttention':
        return SignOrientedAttentionNetwork(
            num_classes=len(args.label_cols),
            backbone=args.backbone,
            feature_dim=args.feature_dim
        )
    elif args.model == 'MambaVision':
        return SignOrientedMambaVisionNetwork(
            num_classes=len(args.label_cols),
            mamba_vision_model=args.mamba_vision_model,
            feature_dim=args.feature_dim,
            pretrained=args.pretrained,
            freeze_backbone=args.freeze_backbone
        )
    
    elif args.model == 'SimpleMambaVision':
        return SimpleMambaVision(
            num_classes=len(args.label_cols),
            mamba_vision_model=args.mamba_vision_model,
            pretrained=args.pretrained,
            freeze_backbone=args.freeze_backbone
        )
    
    elif args.model == 'MambaVisionFusion':
        return MambaVisionWithMambaFusion(
            num_classes=len(args.label_cols),
            mamba_vision_model=args.mamba_vision_model,
            feature_dim=args.feature_dim,
            d_state=args.d_state,
            pretrained=args.pretrained,
            freeze_backbone=args.freeze_backbone
        )
    
    else:
        raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹æ¶æ§‹: {args.model}")

# ğŸ”¥ --- æŠŠé€™æ®µéºæ¼çš„ç¨‹å¼ç¢¼è²¼åœ¨é€™è£¡ --- ğŸ”¥
def clear_gpu_memory():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()

# --- è¤‡è£½è‡ª train.py ---
@torch.no_grad()
def evaluate_model_and_find_thresholds(model, data_loader, device, label_cols, threshold_grid=None):
    if threshold_grid is None:
        threshold_grid = np.arange(0.1, 0.95, 0.05)
    model.eval()
    all_probs, all_labels = [], []
    loop = tqdm(data_loader, desc="Validating", leave=False)
    for (x_whole, x_root, x_center, x_side, x_tip), labels in loop:
        logits = model(
            x_whole.to(device), x_root.to(device), x_center.to(device), 
            x_side.to(device), x_tip.to(device)
        )
        probs = torch.sigmoid(logits).float().cpu().numpy()
        all_probs.append(probs)
        all_labels.append(labels.cpu().numpy())
    if not all_probs: return None, None
    P = np.vstack(all_probs); Y = np.vstack(all_labels); C = Y.shape[1]
    best_thresholds = np.zeros(C, dtype=np.float32)
    best_f1_per_class = np.zeros(C, dtype=np.float32)
    for i in range(C):
        if Y[:, i].sum() == 0:
            best_thresholds[i], best_f1_per_class[i] = 0.5, 0.0
            continue
        best_f1, best_th = -1.0, 0.5
        for th in threshold_grid:
            pred = (P[:, i] > th).astype(int)
            f1 = f1_score(Y[:, i], pred, zero_division=0)
            if f1 > best_f1: best_f1, best_th = f1, th
        best_thresholds[i], best_f1_per_class[i] = best_th, best_f1
    preds_best = np.array([P[:, i] > best_thresholds[i] for i in range(C)]).T
    metrics_at_best = {
        'f1_macro': np.mean(best_f1_per_class).item(),
        'subset_acc': accuracy_score(Y, preds_best)
    }
    return metrics_at_best, best_thresholds

# -------------------------------------------------------------------------
# ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼štrain_model å‡½å¼
# -------------------------------------------------------------------------

def train_model(student_args, current_fold, train_csv, val_csv, student_model_path):
    clear_gpu_memory()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # --- 1. è¼‰å…¥ä¸¦å‡çµ Teacher (è€å¸«) æ¨¡å‹ ---
    print("="*60)
    print(f"ğŸ‘¨â€ğŸ« è¼‰å…¥ Teacher æ¨¡å‹ (Fold {current_fold})...")
    
    # å»ºç«‹ teacher åƒæ•¸
    teacher_args = copy.deepcopy(student_args)
    teacher_args.model = student_args.teacher_model
    teacher_args.backbone = student_args.teacher_backbone
    
    try:
        teacher_model = get_model(teacher_args).to(device)
        
        # å»ºç«‹è€å¸«æ¨¡å‹çš„è·¯å¾‘
        teacher_exp_name = f"{teacher_args.model}_{teacher_args.backbone}"
        teacher_path = f"{teacher_exp_name}_fold{current_fold}.pth"
        
        if not os.path.exists(teacher_path):
            print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° Teacher æ¨¡å‹æ¬Šé‡: {teacher_path}")
            return -1
            
        teacher_model.load_state_dict(torch.load(teacher_path, map_location=device)['model_state_dict'])
        teacher_model.eval() # è¨­ç‚ºè©•ä¼°æ¨¡å¼
        for param in teacher_model.parameters(): # å‡çµæ‰€æœ‰åƒæ•¸
            param.requires_grad = False
            
        print(f"  âœ“ æˆåŠŸè¼‰å…¥ Teacher: {teacher_path}")
    except Exception as e:
        print(f"âŒ è¼‰å…¥ Teacher æ¨¡å‹å¤±æ•—: {e}")
        return -1

    # --- 2. å»ºç«‹ Student (å­¸ç”Ÿ) æ¨¡å‹ ---
    print(f"ğŸ§‘â€ğŸ“ å»ºç«‹ Student æ¨¡å‹ (Fold {current_fold})...")
    # student_args å°±æ˜¯æˆ‘å€‘çš„ä¸» args
    student_model = get_model(student_args).to(device)
    
    # è¨ˆç®—æ¨¡å‹åƒæ•¸é‡ (å­¸ç”Ÿçš„)
    total_params = sum(p.numel() for p in student_model.parameters())
    trainable_params = sum(p.numel() for p in student_model.parameters() if p.requires_grad)
    print(f"  ğŸ“Š Student Model Parameters: {total_params:,} total, {trainable_params:,} trainable")

    # --- 3. æº–å‚™è³‡æ–™ ---
    train_set = TongueDataset(train_csv, student_args.image_root, student_args.label_cols, is_train=True)
    sampler, n_pos_t, n_neg_t = build_multilabel_weighted_sampler(train_csv, student_args.label_cols)
    train_loader = DataLoader(
        train_set, batch_size=student_args.batch_size, sampler=sampler, 
        num_workers=student_args.num_workers, pin_memory=True, drop_last=True
    )
    
    val_set = TongueDataset(val_csv, student_args.image_root, student_args.label_cols, is_train=False)
    val_loader = DataLoader(
        val_set, batch_size=student_args.batch_size * 2, shuffle=False, 
        num_workers=student_args.num_workers, pin_memory=True
    )

    # --- 4. å®šç¾© Loss å‡½å¼ ---
    
    # A. Hard Loss (å­¸ç”Ÿ vs. æ¨™æº–ç­”æ¡ˆ)
    if ClassBalancedBCELoss is not None:
        criterion_hard = ClassBalancedBCELoss(
            n_pos=n_pos_t.to(device), n_neg=n_neg_t.to(device), beta=0.9999
        )
    else:
        criterion_hard = torch.nn.BCEWithLogitsLoss()
        
    # B. Soft Loss (å­¸ç”Ÿ vs. è€å¸«)
    criterion_soft = nn.KLDivLoss(reduction='batchmean')
    
    # è®€å– KD è¶…åƒæ•¸
    T = student_args.temperature
    alpha = student_args.alpha
    print(f"  ğŸ”¥ è’¸é¤¾åƒæ•¸: Alpha={alpha} (å­¸è€å¸«çš„æ¬Šé‡), Temperature={T}")
    print("="*60)
        
    # --- 5. è¨­å®šå„ªåŒ–å™¨ (åªå„ªåŒ–å­¸ç”Ÿ) ---
    optimizer = optim.AdamW(student_model.parameters(), lr=student_args.lr, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=10, T_mult=2, eta_min=1e-7
    )
    scaler = GradScaler()
    
    best_f1_macro, patience_counter = -1.0, 0

    # --- 6. è¨“ç·´è¿´åœˆ ---
    for epoch in range(student_args.epochs):
        student_model.train() # ç¢ºä¿å­¸ç”Ÿæ˜¯è¨“ç·´æ¨¡å¼
        total_loss, total_loss_hard, total_loss_soft = 0.0, 0.0, 0.0
        n_batches = 0
        
        loop = tqdm(train_loader, desc=f"Epoch {epoch+1}/{student_args.epochs} Training", leave=False)
        
        for (x_whole, x_root, x_center, x_side, x_tip), labels in loop:
            # æº–å‚™è¼¸å…¥ (å…¨éƒ¨ä¸Š device)
            x_whole_d = x_whole.to(device)
            x_root_d = x_root.to(device)
            x_center_d = x_center.to(device)
            x_side_d = x_side.to(device)
            x_tip_d = x_tip.to(device)
            labels_d = labels.to(device)
            
            # A. å–å¾— Teacher çš„ Logits (ä¸è¨ˆç®—æ¢¯åº¦)
            with torch.no_grad():
                teacher_logits = teacher_model(
                    x_whole_d, x_root_d, x_center_d, x_side_d, x_tip_d
                )
            
            # B. å–å¾— Student çš„ Logits (è¨ˆç®—æ¢¯åº¦)
            with autocast():
                student_logits = student_model(
                    x_whole_d, x_root_d, x_center_d, x_side_d, x_tip_d
                )
                
                # C. è¨ˆç®— Hard Loss (å­¸ç”Ÿ vs ç­”æ¡ˆ)
                loss_hard = criterion_hard(student_logits, labels_d)
                
                # D. è¨ˆç®— Soft Loss (å­¸ç”Ÿ vs è€å¸«)
                loss_soft = criterion_soft(
                    F.log_softmax(student_logits / T, dim=1),
                    F.softmax(teacher_logits / T, dim=1)
                ) * (T * T) # ä¹˜ T^2 ä¾†é‚„åŸæ¢¯åº¦å°ºåº¦
                
                # E. è¨ˆç®—ç¸½ Loss
                loss = (alpha * loss_soft) + ((1.0 - alpha) * loss_hard)
            
            # F. åå‘å‚³æ’­ (åªæ›´æ–°å­¸ç”Ÿ)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
            
            total_loss += loss.item()
            total_loss_hard += loss_hard.item()
            total_loss_soft += loss_soft.item()
            n_batches += 1
            loop.set_postfix(
                Loss=total_loss/n_batches, 
                Hard=total_loss_hard/n_batches, 
                Soft=total_loss_soft/n_batches
            )
        
        scheduler.step()

        # --- 7. é©—è­‰è¿´åœˆ (åªé©—è­‰å­¸ç”Ÿ) ---
        metrics, best_thresholds = evaluate_model_and_find_thresholds(
            student_model, val_loader, device, student_args.label_cols
        )
        if metrics:
            print(f"\nEpoch {epoch+1}/{student_args.epochs} | Train Loss: {total_loss/n_batches:.4f} | "
                  f"Val F1-Macro: {metrics['f1_macro']:.4f} | Val Subset-Acc: {metrics['subset_acc']:.4f}")
            
            if metrics['f1_macro'] > best_f1_macro:
                best_f1_macro = metrics['f1_macro']
                state = {'model_state_dict': student_model.state_dict()}
                torch.save(state, student_model_path) # å„²å­˜å­¸ç”Ÿæ¨¡å‹
                
                th_path = os.path.splitext(student_model_path)[0] + "_best_thresholds.json"
                with open(th_path, "w", encoding="utf-8") as f:
                    json.dump(
                        {label: float(th) for label, th in zip(student_args.label_cols, best_thresholds)}, 
                        f, indent=2
                    )
                print(f"âœ… Saved best STUDENT model to {os.path.basename(student_model_path)}")
                patience_counter = 0
            else:
                patience_counter += 1
                if patience_counter >= student_args.patience:
                    print(f"Early stopping at epoch {epoch+1}")
                    break
    
    return best_f1_macro


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='çŸ¥è­˜è’¸é¤¾ K-Fold è¨“ç·´è…³æœ¬')
    
    # --- å­¸ç”Ÿ (Student) æ¨¡å‹åƒæ•¸ ---
    parser.add_argument('--model', type=str, required=True,
                        choices=[
                            'Simple', 'SignOriented', 'SignOrientedAttention',
                            'MambaVision', 'SimpleMambaVision', 'MambaVisionFusion'
                        ],
                        help='(å­¸ç”Ÿ) è¦è¨“ç·´çš„è¼•é‡ç´šæ¨¡å‹æ¶æ§‹')
    
    parser.add_argument('--backbone', type=str, default='mobilenetv3_large_100',
                        help='(å­¸ç”Ÿ) è¼•é‡ç´š CNN éª¨å¹¹ç¶²è·¯')
    
    # --- è€å¸« (Teacher) æ¨¡å‹åƒæ•¸ ---
    parser.add_argument('--teacher_model', type=str, required=True,
                        choices=[
                            'Simple', 'SignOriented', 'SignOrientedAttention',
                            'MambaVision', 'SimpleMambaVision', 'MambaVisionFusion'
                        ],
                        help='(è€å¸«) é è¨“ç·´å¥½çš„å¤§å‹æ¨¡å‹æ¶æ§‹')
    
    parser.add_argument('--teacher_backbone', type=str, required=True,
                        help='(è€å¸«) å¤§å‹ CNN éª¨å¹¹ç¶²è·¯ (ä¾‹å¦‚ swin_base_patch4_window7_224)')

    # --- è’¸é¤¾ (Distillation) åƒæ•¸ ---
    parser.add_argument('--alpha', type=float, default=0.7,
                        help='è’¸é¤¾æå¤±(Soft Loss)çš„æ¬Šé‡, alpha=0.7 ä»£è¡¨ 70% å­¸è€å¸«, 30% å­¸ç­”æ¡ˆ')
    parser.add_argument('--temperature', type=float, default=4.0,
                        help='è’¸é¤¾æº«åº¦ T, ç”¨æ–¼å¹³æ»‘ logits (T > 1)')
    
    # --- è¨“ç·´åƒæ•¸ (èˆ‡ train.py ç›¸åŒ) ---
    parser.add_argument('--epochs', type=int, default=30, help='è¨“ç·´ Epoch æ•¸')
    parser.add_argument('--batch_size', type=int, default=16, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=1e-5, help='å­¸ç¿’ç‡')
    parser.add_argument('--patience', type=int, default=7, help='Early stopping è€å¿ƒå€¼')
    parser.add_argument('--feature_dim', type=int, default=512, help='ç‰¹å¾µç¶­åº¦')
    parser.add_argument('--d_state', type=int, default=16, help='Mamba ç‹€æ…‹ç©ºé–“ç¶­åº¦')
    parser.add_argument('--num_mamba_layers', type=int, default=2, help='Mamba å±¤æ•¸')
    parser.add_argument('--img_size', type=int, default=224, help='è¼¸å…¥åœ–ç‰‡å¤§å°')
    parser.add_argument('--patch_size', type=int, default=16, help='Patch å¤§å°')
    parser.add_argument('--embed_dim', type=int, default=512, help='Mamba embedding ç¶­åº¦')
    parser.add_argument('--mamba_depth', type=int, default=6, help='Mamba Backbone æ·±åº¦')
    parser.add_argument('--image_root', type=str, default='images', help='åœ–ç‰‡æ ¹ç›®éŒ„')
    parser.add_argument('--num_workers', type=int, default=4, help='è³‡æ–™è¼‰å…¥ç·šç¨‹æ•¸')
    parser.add_argument('--mamba_vision_model', type=str, default='mamba_vision_T',
                        choices=['mamba_vision_T', 'mamba_vision_T2', 
                                 'mamba_vision_S', 'mamba_vision_B', 'mamba_vision_L'],
                        help='(å­¸ç”Ÿ) MambaVision æ¨¡å‹å¤§å°')
    parser.add_argument('--pretrained', action='store_true', default=True)
    parser.add_argument('--freeze_backbone', action='store_true', default=False)
    
    args = parser.parse_args()

    # æ¨™ç±¤æ¬„ä½
    args.label_cols = ['TonguePale', 'TipSideRed', 'Spot', 'Ecchymosis',
                       'Crack', 'Toothmark', 'FurThick', 'FurYellow']
    NUM_FOLDS = 5

    # --- ğŸ”¥ å»ºç«‹å­¸ç”Ÿçš„å¯¦é©—åç¨± ---
    # å­¸ç”Ÿæ¨¡å‹ (æˆ‘å€‘ä¸»è¦é—œå¿ƒçš„)
    if args.model in ['Simple', 'SignOriented', 'SignOrientedAttention']:
        student_base_name = f'{args.model}_{args.backbone}'
    elif args.model in ['MambaVision', 'SimpleMambaVision', 'MambaVisionFusion']:
        student_base_name = f'{args.model}_{args.mamba_vision_model}'
    else:
        student_base_name = f'{args.model}_{args.backbone}'
    
    # åŠ ä¸Šè’¸é¤¾æ¨™è¨˜
    experiment_name = f"{student_base_name}_KD" # ä¾‹å¦‚: SignOrientedAttention_mobilenetv3_large_100_KD
    
    print(f"ğŸ“¦ å¯¦é©—åç¨± (å°‡ç”¨æ–¼å­˜æª”): {experiment_name}")
    print(f"ğŸ‘¨â€ğŸ« Teacher: {args.teacher_model}_{args.teacher_backbone}")
    print(f"ğŸ§‘â€ğŸ“ Student: {student_base_name}")
    print("="*70)

    # --- K-Fold è¿´åœˆ (èˆ‡ train.py ç›¸åŒ) ---
    
    # Phase 1: K-Fold Cross-Validation
    print("\n" + "="*30 + "\n   PHASE 1: K-Fold CV (Distillation)\n" + "="*30)
    for i in range(1, NUM_FOLDS + 1):
        train_csv = f'train_fold{i}.csv'
        val_csv = f'val_fold{i}.csv'
        
        if not all(os.path.exists(f) for f in [train_csv, val_csv]):
            print(f"âš ï¸   Skipping Fold {i}: CSV files not found")
            continue
        
        model_path = f'{experiment_name}_fold{i}.pth'
        
        print(f"\n{'='*60}")
        print(f"   Training Fold {i}/{NUM_FOLDS}")
        print(f"{'='*66}")
        
        # å‚³å…¥æ‰€æœ‰ argsï¼Œä»¥åŠç•¶å‰çš„ fold
        train_model(args, i, train_csv, val_csv, model_path)

    # Phase 3: Average Thresholds
    print("\n" + "="*30 + "\n   PHASE 3: Average Thresholds\n" + "="*30)
    all_thresholds = []
    
    for i in range(1, NUM_FOLDS + 1):
        th_path = f'{experiment_name}_fold{i}_best_thresholds.json'
        
        if os.path.exists(th_path):
            with open(th_path, 'r', encoding='utf-8') as f:
                fold_ths = json.load(f)
                all_thresholds.append([fold_ths.get(label, 0.5) for label in args.label_cols])
                print(f"âœ“ Loaded {os.path.basename(th_path)}")
    
    if all_thresholds:
        avg_thresholds = {
            label: float(th) 
            for label, th in zip(args.label_cols, np.mean(all_thresholds, axis=0))
        }
        
        final_th_path = f'{experiment_name}_final_best_thresholds.json'
        
        with open(final_th_path, 'w', encoding='utf-8') as f:
            json.dump(avg_thresholds, f, indent=2)
        
        print(f"\nâœ… Averaged thresholds saved to: {final_th_path}")
        for label, th in avg_thresholds.items():
            print(f"   - {label:<15}: {th:.4f}")
    
    print("\n" + "="*70)
    print("   âœ… çŸ¥è­˜è’¸é¤¾ K-Fold è¨“ç·´å®Œæˆï¼")
    print("="*70)
